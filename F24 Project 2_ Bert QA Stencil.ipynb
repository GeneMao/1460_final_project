{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ltpbcm9393_ZK96Glbr1RrMxFaq0vOc0","timestamp":1732700231198}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"35ee01332cb945aa8139861b53f5b8cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04c90a1a60484a4d8d1e866c58b1d1aa","IPY_MODEL_1e19ccd6e8f24d13a4322f0f98c0c9b9","IPY_MODEL_0cb9eef3991246a8ac31262c1620dc57"],"layout":"IPY_MODEL_c5144ee06f424b43af01b6ccaafd70df"}},"04c90a1a60484a4d8d1e866c58b1d1aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97df9f453f61491fb1c3acccabbf66fe","placeholder":"​","style":"IPY_MODEL_dde3ace33e804854a0afce5c17872b8e","value":"100%"}},"1e19ccd6e8f24d13a4322f0f98c0c9b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_519347f00b284558a08ec7898594056b","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_666c94e3c95f4d28b4b1dc3d8bce8667","value":28}},"0cb9eef3991246a8ac31262c1620dc57":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_298526010e0f4e9d9dfac201d002970a","placeholder":"​","style":"IPY_MODEL_13f9200a4144467a8528438c1ea1ef55","value":" 28/28 [00:17&lt;00:00,  1.94it/s]"}},"c5144ee06f424b43af01b6ccaafd70df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97df9f453f61491fb1c3acccabbf66fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dde3ace33e804854a0afce5c17872b8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"519347f00b284558a08ec7898594056b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"666c94e3c95f4d28b4b1dc3d8bce8667":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"298526010e0f4e9d9dfac201d002970a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13f9200a4144467a8528438c1ea1ef55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install -U datasets\n","#load_dataset sometimes hangs on a higher version\n","!pip install transformers"],"metadata":{"id":"9yRIhS6lOTYX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734196445133,"user_tz":-480,"elapsed":6371,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}},"outputId":"99d27f29-2ed9-452a-9af3-674c8b51a579"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]}]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"JifsBqXxmtqm"}},{"cell_type":"code","source":["from datasets import load_dataset\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import random\n","from collections import Counter\n","from transformers import DistilBertModel, DistilBertTokenizerFast\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","\n","seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True"],"metadata":{"id":"OjX3Y-kCzdpE","executionInfo":{"status":"ok","timestamp":1734196445134,"user_tz":-480,"elapsed":4,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0gyjKyGkGL5","executionInfo":{"status":"ok","timestamp":1734196449994,"user_tz":-480,"elapsed":4863,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}},"outputId":"3bac8763-ce08-4e9e-b281-03152bce5aee"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Some options for BERT model that can be run in colab:\n","\n","\"distilbert-base-uncased\",\n","\"distilbert-base-uncased-distilled-squad\",\n","\"distilbert-base-cased\",\n","\"distilbert-base-cased-distilled-squad\",\n","\n","\"\"\""],"metadata":{"id":"gJOgVaQtwRgj","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1734196449995,"user_tz":-480,"elapsed":11,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}},"outputId":"f338ab69-a391-4d55-ce9b-67f4148e6918"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nSome options for BERT model that can be run in colab:\\n\\n\"distilbert-base-uncased\",\\n\"distilbert-base-uncased-distilled-squad\",\\n\"distilbert-base-cased\",\\n\"distilbert-base-cased-distilled-squad\",\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["FOLDER = \"/content/drive/My Drive/Colab Notebooks/CSCI1460/Final\"\n","FILEPATH_TRAIN = f\"{FOLDER}/all_train.json\"\n","FILEPATH_DEV = f\"{FOLDER}/all_dev.json\"\n","data_files = {\"train\": FILEPATH_TRAIN, \"dev\": FILEPATH_DEV}\n","dataset = load_dataset('json', data_files=data_files)"],"metadata":{"id":"dR-hK4i_c0qJ","executionInfo":{"status":"ok","timestamp":1734196449995,"user_tz":-480,"elapsed":8,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["def load_data():\n","    train = dataset[\"train\"]\n","    validation = dataset[\"dev\"]\n","    return train, validation"],"metadata":{"id":"w_JwVr5XkYMj","executionInfo":{"status":"ok","timestamp":1734196449995,"user_tz":-480,"elapsed":8,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["def preprocess_and_tokenize(data, tokenizer, max_length=384, batch_size=64):\n","    '''\n","    This function preprocesses and tokenizes the given dataset for question answering tasks.\n","    Parameters:\n","        data (list): A list of dictionaries containing 'questions', 'contexts', and 'answers'.\n","        tokenizer: The tokenizer to be used, which should be compatible with the transformer model in use.\n","        max_length (int, optional): The maximum number of tokens for each encoded example. Defaults to 384.\n","        batch_size (int, optional): The batch size for the DataLoader. Defaults to 64.\n","\n","    Returns:\n","        DataLoader: A PyTorch DataLoader containing tokenized and encoded data ready for training.\n","    '''\n","    class MyQADataset(torch.utils.data.Dataset):\n","        def __init__(self, data, tokenizer, max_length=384):\n","            self.tokenizer = tokenizer\n","            self.max_length = max_length\n","            self.data = data\n","\n","            self.tokenizer.add_special_tokens({\n","                \"cls_token\": \"<CLS>\",\n","                \"sep_token\": \"<SEP>\",\n","            })\n","\n","            self.input_ids_list = []\n","            self.attention_mask_list = []\n","            self.start_pos_list = []\n","            self.end_pos_list = []\n","            self.answer_label_list = []\n","\n","            for example in data:\n","                input_ids, attention_mask, start_pos, end_pos, answer_label = self._encode_example(example)\n","                self.input_ids_list.append(input_ids)\n","                self.attention_mask_list.append(attention_mask)\n","                self.start_pos_list.append(start_pos)\n","                self.end_pos_list.append(end_pos)\n","                self.answer_label_list.append(answer_label)\n","\n","        def _encode_example(self, example):\n","            question = example['questions'][0]['input_text']\n","            context = example['contexts']\n","            answer = example['answers']\n","\n","            encodings = self.tokenizer(\n","                question,\n","                context,\n","                max_length=self.max_length,\n","                padding='max_length',\n","                return_attention_mask=True,\n","                return_offsets_mapping=True,\n","                return_token_type_ids=True,\n","                truncation=True\n","            )\n","\n","            start_char_idx = answer[0]['span_start']\n","            end_char_idx = answer[0]['span_end']\n","            answer_label = 1 if answer[0]['input_text'] == 'short' else 0\n","            offset_mapping = encodings['offset_mapping']\n","            token_type_ids = encodings['token_type_ids']\n","\n","            start_token_idx, end_token_idx = 0, 0\n","            ctx_start_idx = token_type_ids.index(1)\n","            ctx_end_idx = len(token_type_ids) - token_type_ids[::-1].index(1) - 1\n","\n","            if answer_label == 1:\n","                for i, offset in enumerate(offset_mapping[ctx_start_idx:ctx_end_idx+1], start=ctx_start_idx):\n","                    if offset[0] <= start_char_idx < offset[1]:\n","                        start_token_idx = i\n","                    if offset[0] < end_char_idx <= offset[1]:\n","                        end_token_idx = i\n","                    if start_token_idx != 0 and end_token_idx != 0:\n","                        break\n","\n","            return (\n","                torch.tensor(encodings['input_ids']),\n","                torch.tensor(encodings['attention_mask']),\n","                torch.tensor(start_token_idx),\n","                torch.tensor(end_token_idx),\n","                torch.tensor(answer_label)\n","            )\n","\n","        def __len__(self):\n","            return len(self.data)\n","\n","        def __getitem__(self, idx):\n","            return {\n","                'input_ids': self.input_ids_list[idx],\n","                'attention_mask': self.attention_mask_list[idx],\n","                'start_pos': self.start_pos_list[idx],\n","                'end_pos': self.end_pos_list[idx],\n","                'answer_label': self.answer_label_list[idx],\n","            }\n","\n","    dataset = MyQADataset(data, tokenizer, max_length=max_length)\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    return data_loader"],"metadata":{"id":"JNiAEkMikYF1","executionInfo":{"status":"ok","timestamp":1734196449995,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["class MyBertQAModel(nn.Module):\n","    '''\n","    This class defines a BERT-based model for question answering tasks. It utilizes a DistilBert model as the backbone for feature extraction.\n","    Parameters:\n","        pretrained_model_name (str, optional): The name of the pre-trained BERT model to use. Defaults to \"distilbert-base-uncased\".\n","\n","    Returns:\n","        tuple: A tuple containing three tensors:\n","            - start_logits (torch.Tensor): Logits for the start positions of the answers [batch, seq_len].\n","            - end_logits (torch.Tensor): Logits for the end positions of the answers [batch, seq_len].\n","            - answer_logits (torch.Tensor): Logits for the type of the answers [batch, 2].\n","    '''\n","    def __init__(self, pretrained_model_name=\"distilbert-base-uncased\"):\n","        super(MyBertQAModel, self).__init__()\n","        self.model = DistilBertModel.from_pretrained(pretrained_model_name)\n","        hidden_size = self.model.config.hidden_size\n","        self.start_layer = nn.Linear(hidden_size, 1)\n","        self.end_layer = nn.Linear(hidden_size, 1)\n","        self.answer_layer = nn.Linear(hidden_size, 2)\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        sequence_output = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n","        cls_output = sequence_output[:, 0, :]        # [batch, hidden_size]\n","\n","        start_logits = self.start_layer(sequence_output).squeeze(-1) # [batch, seq_len]\n","        end_logits = self.end_layer(sequence_output).squeeze(-1)     # [batch, seq_len]\n","        answer_logits = self.answer_layer(self.dropout(cls_output))  # [batch, 2]\n","\n","        return start_logits, end_logits, answer_logits\n"],"metadata":{"id":"tpPhMmIzkYUv","executionInfo":{"status":"ok","timestamp":1734196449995,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["def load_model():\n","    model = MyBertQAModel()\n","    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","    return model, tokenizer"],"metadata":{"id":"cSLXI10SkYXX","executionInfo":{"status":"ok","timestamp":1734196449995,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["def calc_metrics(input_ids, start_logits, end_logits, start_positions, end_positions):\n","    \"\"\"\n","    Compute precision, recall, and F1 score for a batch of QA predictions.\n","\n","    Parameters:\n","        input_ids (torch.Tensor): Token IDs for the input sequences. [batch_size, seq_len]\n","        start_logits (torch.Tensor): Logits for predicted start indices. [batch_size, seq_len]\n","        end_logits (torch.Tensor): Logits for predicted end indices. [batch_size, seq_len]\n","        start_positions (torch.Tensor): True start indices of the answer spans. [batch_size]\n","        end_positions (torch.Tensor): True end indices of the answer spans. [batch_size]\n","\n","    Returns:\n","        (float, float, float): A tuple containing the average precision, recall, and F1 score over the batch.\n","    \"\"\"\n","    batch_size = start_logits.size(0)\n","    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","    total_prec, total_rec, total_f1 = 0.0, 0.0, 0.0\n","\n","    for idx in range(batch_size):\n","        pred_start_idx = torch.argmax(start_logits[idx])\n","        pred_end_idx = torch.argmax(end_logits[idx])\n","\n","        true_start_idx = start_positions[idx]\n","        true_end_idx = end_positions[idx]\n","\n","        if pred_start_idx > pred_end_idx:\n","            continue\n","        predicted_tokens = tokenizer.convert_ids_to_tokens(input_ids[idx][pred_start_idx:pred_end_idx+1])\n","        actual_tokens = tokenizer.convert_ids_to_tokens(input_ids[idx][true_start_idx:true_end_idx+1])\n","\n","        pred_counts = Counter(predicted_tokens)\n","        actual_counts = Counter(actual_tokens)\n","        true_positive = sum((pred_counts & actual_counts).values())\n","        false_positive = sum((pred_counts - actual_counts).values())\n","        false_negative = sum((actual_counts - pred_counts).values())\n","        precision_val = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0.0\n","        recall_val = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0.0\n","        f1_val = (2 * precision_val * recall_val / (precision_val + recall_val)) if (precision_val + recall_val) > 0 else 0.0\n","\n","        total_prec += precision_val\n","        total_rec += recall_val\n","        total_f1 += f1_val\n","\n","    avg_precision = total_prec / batch_size\n","    avg_recall = total_rec / batch_size\n","    avg_f1 = total_f1 / batch_size\n","\n","    return avg_precision, avg_recall, avg_f1\n"],"metadata":{"id":"4cf7bOjZJjDL","executionInfo":{"status":"ok","timestamp":1734196450676,"user_tz":-480,"elapsed":688,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["def eval_loop(validation_data_loader, model, tokenizer, device):\n","    '''\n","    Perform evaluation over a validation dataset using a given model. This function iterates over the validation dataset, makes predictions using the model,\n","    and calculates the precision, recall, and F1 score for each batch.\n","    Parameters:\n","        validation_data_loader (DataLoader): DataLoader for the validation dataset.\n","        model (nn.Module): The trained model to be evaluated.\n","        tokenizer: The tokenizer used for processing the data.\n","        device: The device (e.g., 'cuda', 'cpu') on which the tensors should be processed.\n","\n","    Returns:\n","        tuple: A tuple containing the average precision, recall, and F1 score for the validation set.\n","    '''\n","    model.eval()\n","    total_precision, total_recall, total_f1 = 0.0, 0.0, 0.0\n","\n","    print(\"Evaluating metrics:\")\n","    progress_bar = tqdm(range(len(validation_data_loader)))\n","\n","    with torch.no_grad():\n","        for batch in validation_data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            start_positions = batch['start_pos'].to(device)\n","            end_positions = batch['end_pos'].to(device)\n","            answer_labels = batch['answer_label'].to(device)\n","\n","            start_logits, end_logits, answer_logits = model(input_ids, attention_mask)\n","            precision, recall, f1 = calc_metrics(input_ids, start_logits, end_logits, start_positions, end_positions)\n","\n","            total_precision += precision\n","            total_recall += recall\n","            total_f1 += f1\n","\n","            progress_bar.update(1)\n","\n","    precision = total_precision / len(validation_data_loader)\n","    recall = total_recall / len(validation_data_loader)\n","    f1_score = total_f1 / len(validation_data_loader)\n","    return precision, recall, f1_score\n"],"metadata":{"id":"9xyF9HisuLWd","executionInfo":{"status":"ok","timestamp":1734196450676,"user_tz":-480,"elapsed":4,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["def train_loop(train_data_loader, validation_data_loader, model, device, epochs=2, lr=5e-5):\n","    '''\n","    Conducts the training process over a specified number of epochs. This function iterates over the training data to train the model,\n","    computes loss using a defined function within the loop, and performs validation at the end of each epoch to monitor performance.\n","    Parameters:\n","        train_data_loader (DataLoader): DataLoader for the training dataset.\n","        validation_data_loader (DataLoader): DataLoader for the validation dataset.\n","        model (nn.Module): The model to be trained.\n","        device: The device (e.g., 'cuda', 'cpu') on which to process the model and data.\n","        epochs (int, optional): The number of epochs to train for. Defaults to 2.\n","        lr (float, optional): Learning rate for the optimizer. Defaults to 5e-5.\n","\n","    Returns:\n","        list: Lists containing the average training and validation loss for each epoch.\n","    '''\n","    model.to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","\n","    def cal_loss(start_logits, end_logits, answer_logits, start_positions, end_positions, answer_labels):\n","        start_loss = nn.CrossEntropyLoss()(start_logits, start_positions)\n","        end_loss = nn.CrossEntropyLoss()(end_logits, end_positions)\n","        answer_loss = nn.CrossEntropyLoss()(answer_logits, answer_labels)\n","        return start_loss + end_loss + answer_loss\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        epoch_train_loss = 0\n","        for batch in train_data_loader:\n","            optimizer.zero_grad()\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            start_positions = batch['start_pos'].to(device)\n","            end_positions = batch['end_pos'].to(device)\n","            answer_labels = batch['answer_label'].to(device)\n","\n","            start_logits, end_logits, answer_logits = model(input_ids, attention_mask)\n","            loss = cal_loss(start_logits, end_logits, answer_logits, start_positions, end_positions, answer_labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_train_loss += loss.item()\n","\n","        avg_train_loss = epoch_train_loss / len(train_data_loader)\n","\n","        model.eval()\n","        epoch_val_loss = 0\n","        with torch.no_grad():\n","            for batch in validation_data_loader:\n","                input_ids = batch[\"input_ids\"].to(device)\n","                attention_mask = batch[\"attention_mask\"].to(device)\n","                start_positions = batch[\"start_pos\"].to(device)\n","                end_positions = batch[\"end_pos\"].to(device)\n","                answer_labels = batch[\"answer_label\"].to(device)\n","\n","                start_logits, end_logits, answer_logits = model(input_ids, attention_mask)\n","                val_loss = cal_loss(start_logits, end_logits, answer_logits, start_positions, end_positions, answer_labels)\n","                epoch_val_loss += val_loss.item()\n","\n","        avg_val_loss = epoch_val_loss / len(validation_data_loader)\n","        print(f\"Epoch: {epoch}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","\n","    return [avg_train_loss], [avg_val_loss]"],"metadata":{"id":"OtqCRoqxuN7M","executionInfo":{"status":"ok","timestamp":1734196450676,"user_tz":-480,"elapsed":3,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["def main():\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    batch_size = 64\n","    max_length = 384\n","\n","    model, tokenizer = load_model()\n","    model.to(device)\n","    train, validation = load_data()\n","\n","    train_data_loader = preprocess_and_tokenize(train, tokenizer, max_length=max_length, batch_size=batch_size)\n","    validation_data_loader = preprocess_and_tokenize(validation, tokenizer, max_length=max_length, batch_size=batch_size)\n","\n","    train_losses, val_losses = train_loop(train_data_loader, validation_data_loader, model, device, epochs=2, lr=5e-5)\n","    precision, recall, f1_score = eval_loop(validation_data_loader, model, tokenizer, device)\n","\n","    print(\"PRECISION: \", precision)\n","    print(\"RECALL: \", recall)\n","    print(\"F1-SCORE: \", f1_score)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["35ee01332cb945aa8139861b53f5b8cf","04c90a1a60484a4d8d1e866c58b1d1aa","1e19ccd6e8f24d13a4322f0f98c0c9b9","0cb9eef3991246a8ac31262c1620dc57","c5144ee06f424b43af01b6ccaafd70df","97df9f453f61491fb1c3acccabbf66fe","dde3ace33e804854a0afce5c17872b8e","519347f00b284558a08ec7898594056b","666c94e3c95f4d28b4b1dc3d8bce8667","298526010e0f4e9d9dfac201d002970a","13f9200a4144467a8528438c1ea1ef55"]},"id":"iLXGRBbT1o1l","executionInfo":{"status":"ok","timestamp":1734198281737,"user_tz":-480,"elapsed":1831064,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}},"outputId":"6e8fb3bb-eab8-4db6-b92a-c0d1985ede85"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Train Loss: 3.8255, Val Loss: 2.7802\n","Epoch: 1, Train Loss: 2.1544, Val Loss: 2.4885\n","Evaluating metrics:\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/28 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35ee01332cb945aa8139861b53f5b8cf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["PRECISION:  0.7052367129857312\n","RECALL:  0.7198980327241281\n","F1-SCORE:  0.6880363161681837\n"]}]},{"cell_type":"code","source":["# def main():\n","#   '''Here's the basic structure of the main block -- feel free to add or\n","#   remove parameters/helper functions as you see fit, but all steps here are\n","#   needed and we expect to see precision, recall, and f1 scores printed out'''\n","#   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","#   batch_size = 64\n","\n","#   model, tokenizer = load_model()\n","#   train, validation = load_data()\n","\n","#   train_data_loader = preprocess_and_tokenize(train)\n","#   validation_data_loader = preprocess_and_tokenize(validation)\n","\n","#   train_losses, val_losses = train_loop(train_data_loader, validation_data_loader)\n","#   precision, recall, f1_score  = eval_loop(validation_data_loader)\n","\n","#   print(\"PRECISION: \", precision)\n","#   print(\"RECALL: \", recall)\n","#   print(\"F1-SCORE: \", f1_score)\n","\n","# if __name__ == \"__main__\":\n","#   main()\n"],"metadata":{"id":"KS9ZcP-umJLN","executionInfo":{"status":"ok","timestamp":1734198281737,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yuhan Mao","userId":"01286054096939291718"}}},"execution_count":65,"outputs":[]}]}