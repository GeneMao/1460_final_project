# 1460_final_project

In this project, a refined DistilBERT model is used to create a question-answering system. It finds a brief answer span from the context or concludes that there isn't one after receiving a question and a section of context. The work on BERT for Natural Questions served as the inspiration for the code's streamlined methodology. Data loading from JSON files, tokenization, character-level response mapping to token locations, and training/evaluation are all included. Predicted and ground-truth token spans are compared to calculate precision, recall, and F1 measures.

Video link: https://drive.google.com/file/d/1rLkBnz5bz2E1VFT-S4aVN9HTAfnFCrvg/view